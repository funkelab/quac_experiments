# %% 
# # Visualize Lesion Segmentation Results
# In this notebook, we pick up all of the parts that were done in the previous two notebooks, 
# and turn them into data for figures. 
# 
# We will begin as always by loading the metadata. 
# %%
import yaml
metadata = yaml.safe_load(open("lesion_segmentation.yaml"))

# %% [markdown]
# ## Getting the reference image of the counterfactual
#
# The counterfactuals we are using in this notebook were generated by-reference. 
# This means that they were made using a style image that was taken from another image.
# At the moment, the name of reference image is stored in a text file the same directory as the counterfactuals.
# We will get this file, so that we can load the refernce image with the counterfactual.
# %%
# TODO this will be simplified in the future, and the full paths will be a part of the report.
import os
import pandas as pd

reference_metadata = []
for dirpath, dirname, filenames in os.walk(metadata["counterfactual_directory"]):
    for filename in filenames:
        if filename.endswith(".txt"):
            reference_metadata.append(os.path.join(dirpath, filename))

reference_df = pd.read_csv(reference_metadata[0], header=None, sep=" ")
reference_df.columns=["counterfactual", "reference"]
# %% [markdown]
# Next, we specify the path of these reference images, so that we can load them. 
# Once again, this will not be necessary in the future. 

# %%
metadata["reference_directory"] = "/nrs/funke/adjavond/data/retina/ddrdataset/DR_grading/DR_grading_processed/val/0_No_DR"

# %% [markdown]
# We will add the full path to the reference images, so that we can load them easily.
# Additionally, we set the counterfactual's name as an index to be easier to use.
reference_df["reference"] = reference_df["reference"].map(lambda x: os.path.join(metadata["reference_directory"], x))
reference_df.set_index("counterfactual", inplace=True)

# %% [markdown]
# ## Loading the reports
# The first thing we want to do is load the different reports, so that we can compare the 
# different attribution methods.
# 
# We will plot them, and then choose the best one to save.

# %%
from quac.report import Report
reports = {
    method: Report(name=method) for method in ["discriminative_deeplift", "vanilla_deeplift", "discriminative_ig", "vanilla_ig"]
}

for method, report in reports.items():
    report.load(metadata["report_directory"] + "/" + method + "/default.json")

# %% [markdown]
# ## Plotting the curves
# We will now plot the curves for the different methods on the same figure.

# %%
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
for method, report in reports.items():
    report.plot_curve(ax=ax)
# Add the legend
plt.legend()
plt.show()

# %% [markdown]
# ## Saving the mean and standard deviation of the curves
# For future use, we will save the mean and standard deviation of the curves to a CSV file.
# %%
import numpy as np
curve_file = metadata["figures_directory"] + "/curve.csv"

data = []
for method, report in reports.items():
    mean, std = report.get_curve()
    data.append(mean)
    data.append(std)
data = np.array(data).T

with open(curve_file, "w") as f:
    # Write the method names
    for method in reports:
        f.write(f"{method}_mean,{method}_std,")
    f.write("\n")
    for i in range(len(mean)):
        f.write(",".join(map(str, data[i])) + "\n")

print("Saved to", curve_file)

# %%
# TODO fill in here with the best of the methods
report = reports["discriminative_ig"]
# %% [markdown]
# ## Choosing the best example
# Next we want to choose the best example, given the best method.
# This is done by ordering the examples by the QuAC score, and then choosing the one with the highest score.
#
# %%
import numpy as np
order = np.argsort(report.quac_scores)[::-1]

# %% [markdown]
# We will then load that example and its counterfactual from its path, and visualize it.
# We also want to see the classification of both the original and the counterfactual.

# %%
# Transform to apply to the images so they match each other
from torchvision import transforms
from PIL import Image
transform = transforms.Compose(
    [
        transforms.Resize(224),
        transforms.CenterCrop(224),
    ]
)
# loading
image_path, cf_path = report.paths[order[0]], report.target_paths[order[0]]
image, cf_image = Image.open(image_path), Image.open(cf_path)
image, cf_image = transform(image), transform(cf_image)

prediction = report.predictions[order[0]]
target_prediction = report.target_predictions[order[0]]

# %% [markdown]
# ## Loading the corresponding reference image
# We next use the information that we obtained earlier to load the reference image as well.
from pathlib import Path
reference_path = reference_df.loc[Path(cf_path).name].values[0]
reference_image = Image.open(reference_path)
reference_image = transform(reference_image)
# %% [markdown]
# ## Getting the reference classification
# 
# We will also get the classification of the reference image, so that we can compare it to the counterfactual.
# note:: Once again, this will be simplified in the future.
# %%
import torch
from torch.nn.functional import softmax
from quac.generate import load_classifier

pre_classifier = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize(mean=0.5, std=0.5)
    ]
)

classifier = load_classifier(
    metadata["classifier_checkpoint"], mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)
)

with torch.no_grad():
    reference_pred = softmax(
        classifier(pre_classifier(reference_image).unsqueeze(0))
    ).detach().numpy().flatten()

# %% [markdown]
# ## Visualizing the example
# We will now visualize the example, showing the original image, the counterfactual, and the prediction of both.
# %%
fig, axes = plt.subplots(2, 3)
axes[1, 0].imshow(image)
axes[0, 0].bar(np.arange(len(prediction)), prediction)
axes[1, 1].imshow(cf_image)
axes[0, 1].bar(np.arange(len(target_prediction)), target_prediction)
axes[1, 2].imshow(reference_image)
axes[0, 2].bar(np.arange(len(reference_pred)), reference_pred)
plt.show()
# %% [markdown]
# Let's additionally save the cropped and resized images, and store the classifications in a CSV file
# %%
# Images
image_name = ["image", "counterfactual", "reference"]
figure_path = Path(metadata["figures_directory"])
for name, img in zip(image_name, [image, cf_image, reference_image]):
    img.save(figure_path / (name + ".png"))

# Predictions
with open(figure_path / "predictions.csv", "w") as f:
    f.write("file,None,Mild,Moderate,Severe,Proliferative\n")
    for name, pred in zip(image_name, [prediction, target_prediction, reference_pred]):
        f.write(f"{name}.png,{','.join(map(str, pred))}\n")

print("Saved to", figure_path)
# %% [markdown]
# ## Loading the attribution
# We next want to load the attribution for the example, and visualize it.
# %%
attribution_path = report.attribution_paths[order[0]]
attribution = np.load(attribution_path)

fig, axes = plt.subplots(1, len(attribution))  # For each channel
for i, ax in enumerate(axes):
    ax.imshow(attribution[i], vmin=0, vmax=1)
    ax.set_title(f"Channel {i}")
plt.show()

# %% [markdown]
# ## Getting the processor
# We want to see the specific mask that was optimal in this case.
# To do this, we will need to get the optimal threshold, and get the processor used for masking. 

# %%
from quac.evaluation import Processor

thresh = report.get_optimal_threshold(order[0])
processor = Processor()

mask, _ = processor.create_mask(attribution, thresh)

plt.imshow(np.transpose(mask, (1, 2, 0)))

# %% [markdown]
# ## Comparing the mask to the lesion segmentation
#
# In this particular example, we actually have the lesion segmentation available.
# We will load it, and compare it to the mask we have obtained.
# %%
from utils import load_label, mask_label_overlay

label_types=["EX", "HE", "MA", "SE"]

overlays = []
all_labels = []

fig, axes = plt.subplots(2, 2)
for i, ax in enumerate(axes.flatten()):
    label = load_label(image_path, label_types[i])
    all_labels.append(label)
    rgb = mask_label_overlay(mask, label)
    overlays.append(rgb)
    ax.imshow(rgb)
    ax.set_title(f"{label_types[i]}")
    ax.axis("off")

fig.tight_layout()
# %% [markdown]
# ## Saving the overlay 
# We might want to use these overlay images in the future. 
# We will therefore save them to file.
# 
# We will also need an overlay for the summed labels, so we will create that as well.
# %%
for label, overlay in zip(label_types, overlays):
    overlay_path = figure_path / f"{label}_overlay.png"
    Image.fromarray(overlay).save(overlay_path)

# Summed labels
overlay = mask_label_overlay(mask, np.sum(all_labels, axis=0))
Image.fromarray(overlay).save(figure_path / "summed_overlay.png")
print("Saved overlays to", figure_path)
# %% [markdown]
# ## Getting the detection score overall
# 
# Let's get the precision and recall scores, per label, for each sample.
# 
# We also want to see how much the classifier is looking at that is *not* in the lesion segmentation given by the expert.
# In this case, it doesn't matter *which* label, so we will use the summed labels.
# 
# In order to get these metrics, we need to choose a threshold for the mask.
# In this case, we have chosen 0.1 as a threshold (manually), as this recovers what we consider
# as being "in" the mask. As shown below. 
# %%
fig, ax = plt.subplots(1, 2)
ax[0].imshow(np.mean(mask, axis=0))
ax[1].imshow(np.mean(mask, axis=0) > 0.1)

ax[0].axis("off")
ax[1].axis("off")
# %% [markdown]
# Feel free to change this threshold to see how it affects the metrics.
# %%
from tqdm import tqdm
from utils import iou, precision, recall
precisions = {
    label_type: [] for label_type in label_types
}
recalls = {
    label_type: [] for label_type in label_types
}
f1_scores = {
    label_type: [] for label_type in label_types
} 

mask_sizes = []
for i in tqdm(range(len(report.paths))):
    attribution_path = report.attribution_paths[i]
    attribution = np.load(attribution_path)

    thresh, idx = report.get_optimal_threshold(i, return_index=True)
    mask, _ = processor.create_mask(attribution, thresh)
    mask_sizes.append(report.normalized_mask_sizes[i][idx])
    for j, label_type in enumerate(label_types):
        label = load_label(image_path, label_type)
        p = precision(mask, label, thresh=0.1)
        r = recall(mask, label, thresh=0.1)
        f1 = (2 * p * r )/ max(p + r, 1)
        f1_scores[label_type].append(f1)
        precisions[label_type].append(p)
        recalls[label_type].append(r)

# %% [markdown]
# ## Plotting the Precision and Recall
# We will plot the precision and recall for each label type.
#
# A high recall means that the manual segmentation is well covered by the mask.
# A low precision means that the classifier is *also* looking at things that are not in the manual segmentation.
# %%
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
ax1.hist(list(recalls.values()), bins=20, alpha=0.5, label=label_types)
ax1.set_xlabel("Recall")
ax1.set_ylabel("Count")
ax2.hist(list(precisions.values()), bins=20, alpha=0.5, label=label_types)
ax2.set_xlabel("Precision")
ax2.set_ylabel("Count")
plt.legend()
# %% [markdown]
# ## Saving the precision and recall
# We will save the precision and recall to a CSV file, so that we can use it in the future.
# %%
precision_recall_file = figure_path / "precision_recall.csv"
with open(precision_recall_file, "w") as f:
    f.write("label_type,precision,recall,f1\n")
    for label_type in label_types:
        for p, r, f1 in zip(precisions[label_type], recalls[label_type], f1_scores[label_type]):
            f.write(f"{label_type},{p},{r},{f1}\n")

print("Saved precision and recall to", precision_recall_file)
# %% [markdown]
# ## Plotting mask sizes
# %% 
plt.hist(mask_sizes, bins=20, alpha=0.5)
plt.xlabel("Mask size")
plt.ylabel("Count")
plt.show()
# %%
plt.scatter(mask_sizes, report.quac_scores)

# %%
