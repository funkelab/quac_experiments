# %%
# # Visualize Lesion Segmentation Results
# In this notebook, we pick up all of the parts that were done in the previous two notebooks,
# and turn them into data for figures.
#
# We will begin as always by loading the metadata.
# %%
import yaml

metadata = yaml.safe_load(open("lesion_segmentation.yaml"))

# %% [markdown]
# ## Getting the reference image of the counterfactual
#
# The counterfactuals we are using in this notebook were generated by-reference.
# This means that they were made using a style image that was taken from another image.
# At the moment, the name of reference image is stored in a text file the same directory as the counterfactuals.
# We will get this file, so that we can load the reference image with the counterfactual.
# %%
# TODO this will be simplified in the future, and the full paths will be a part of the report.
import os
import pandas as pd

reference_metadata = []
for dirpath, dirname, filenames in os.walk(metadata["counterfactual_directory"]):
    for filename in filenames:
        if filename.endswith(".txt"):
            reference_metadata.append(os.path.join(dirpath, filename))

reference_df = pd.read_csv(reference_metadata[0], header=None, sep=" ")
reference_df.columns = ["counterfactual", "reference"]
# %% [markdown]
# Next, we specify the path of these reference images, so that we can load them.
# Once again, this will not be necessary in the future.

# %%
metadata["reference_directory"] = (
    "/nrs/funke/adjavond/data/retina/ddrdataset/DR_grading/DR_grading_processed/val/0_No_DR"
)

# %% [markdown]
# We will add the full path to the reference images, so that we can load them easily.
# Additionally, we set the counterfactual's name as an index to be easier to use.
reference_df["reference"] = reference_df["reference"].map(
    lambda x: os.path.join(metadata["reference_directory"], x)
)
reference_df.set_index("counterfactual", inplace=True)

# %% [markdown]
# ## Loading the reports
# The first thing we want to do is load the different reports, so that we can compare the
# different attribution methods.
#
# We will plot them, and then choose the best one to save.

# %%
from quac.report import Report

reports = {
    method: Report(name=method)
    for method in [
        "discriminative_deeplift",
        "vanilla_deeplift",
        "discriminative_ig",
        "vanilla_ig",
    ]
}

for method, report in reports.items():
    report.load(metadata["report_directory"] + "/" + method + "/default.json")

# %% [markdown]
# ## Plotting the curves
# We will now plot the curves for the different methods on the same figure.

# %%
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
for method, report in reports.items():
    report.plot_curve(ax=ax)
# Add the legend
plt.legend()
plt.show()

# %% [markdown]
# ## Saving the median and IQR of the curves
# For future use, we will save the median and IQR of the curves to a CSV file.
# %%
import numpy as np

# curve_file = metadata["figures_directory"] + "/curve.csv"
curve_file = "curve.csv"  # TODO we can move it later

data = []
for method, report in reports.items():
    median, p25, p75 = report.get_curve()
    data.append(median)
    data.append(p25)
    data.append(p75)
data = np.array(data).T

with open(curve_file, "w") as f:
    # Write the method names
    for method in reports:
        f.write(f"{method}_median,{method}_p25,{method}_p75,")
    f.write("\n")
    for i in range(len(data)):
        f.write(",".join(map(str, data[i])) + "\n")

print("Saved to", curve_file)

# %%
# TODO fill in here with the best of the methods
report = reports["discriminative_ig"]
# %% [markdown]
# ## Choosing the best example
# Next we want to choose the best example, given the best method.
# This is done by ordering the examples by the QuAC score, and then choosing the one with the highest score.
#
# %%
import numpy as np

order = np.argsort(report.quac_scores)[::-1]

# %% [markdown]
# We will then load that example and its counterfactual from its path, and visualize it.
# We also want to see the classification of both the original and the counterfactual.

# %%
# Transform to apply to the images so they match each other
from torchvision import transforms
from PIL import Image

transform = transforms.Compose(
    [
        transforms.Resize(224),
        transforms.CenterCrop(224),
    ]
)
# loading
image_path, cf_path = report.paths[order[0]], report.target_paths[order[0]]
image, cf_image = Image.open(image_path), Image.open(cf_path)
image, cf_image = transform(image), transform(cf_image)

prediction = report.predictions[order[0]]
target_prediction = report.target_predictions[order[0]]

# %% [markdown]
# ## Loading the corresponding reference image
# We next use the information that we obtained earlier to load the reference image as well.
from pathlib import Path

reference_path = reference_df.loc[Path(cf_path).name].values[0]
reference_image = Image.open(reference_path)
reference_image = transform(reference_image)
# %% [markdown]
# ## Getting the reference classification
#
# We will also get the classification of the reference image, so that we can compare it to the counterfactual.
# note:: Once again, this will be simplified in the future.
# %%
import torch
from torch.nn.functional import softmax
from quac.generate import load_classifier

pre_classifier = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize(mean=0.5, std=0.5)]
)

classifier = load_classifier(
    metadata["classifier_checkpoint"],
    mean=(0.485, 0.456, 0.406),
    std=(0.229, 0.224, 0.225),
)

with torch.no_grad():
    reference_pred = (
        softmax(classifier(pre_classifier(reference_image).unsqueeze(0)))
        .detach()
        .numpy()
        .flatten()
    )

# %% [markdown]
# ## Visualizing the example
# We will now visualize the example, showing the original image, the counterfactual, and the prediction of both.
# %%
fig, axes = plt.subplots(2, 3)
axes[1, 0].imshow(image)
axes[1, 0].axis("off")
axes[1, 0].set_title("Original")
axes[0, 0].bar(np.arange(len(prediction)), prediction)
axes[1, 1].imshow(cf_image)
axes[1, 1].axis("off")
axes[1, 1].set_title("Counterfactual")
axes[0, 1].bar(np.arange(len(target_prediction)), target_prediction)
axes[1, 2].imshow(reference_image)
axes[1, 2].axis("off")
axes[1, 2].set_title("Reference")
axes[0, 2].bar(np.arange(len(reference_pred)), reference_pred)
plt.show()
# %% [markdown]
# Let's additionally save the cropped and resized images, and store the classifications in a CSV file
# %%
# # Images
# image_name = ["image", "counterfactual", "reference"]
# figure_path = Path(metadata["figures_directory"])
# for name, img in zip(image_name, [image, cf_image, reference_image]):
#     img.save(figure_path / (name + ".png"))

# # Predictions
# with open(figure_path / "predictions.csv", "w") as f:
#     f.write("file,None,Mild,Moderate,Severe,Proliferative\n")
#     for name, pred in zip(image_name, [prediction, target_prediction, reference_pred]):
#         f.write(f"{name}.png,{','.join(map(str, pred))}\n")

# print("Saved to", figure_path)
# %% [markdown]
# ## Loading the attribution
# We next want to load the attribution for the example, and visualize it.
# %%
attribution_path = report.attribution_paths[order[0]]
attribution = np.load(attribution_path)

fig, axes = plt.subplots(1, len(attribution))  # For each channel
for i, ax in enumerate(axes):
    ax.imshow(attribution[i], vmin=0, vmax=1)
    ax.set_title(f"Channel {i}")
plt.show()

# %% [markdown]
# ## Getting the processor
# We want to see the specific mask that was optimal in this case.
# To do this, we will need to get the optimal threshold, and get the processor used for masking.

# %%
from quac.evaluation import Processor

thresh = report.get_optimal_threshold(order[0])
processor = Processor()

mask, _ = processor.create_mask(attribution, thresh)

plt.imshow(np.transpose(mask, (1, 2, 0)))

# %% [markdown]
# ## Comparing the mask to the lesion segmentation
#
# In this particular example, we actually have the lesion segmentation available.
# We will load it, and compare it to the mask we have obtained.
# %%
from utils import load_label, mask_label_overlay

label_types = ["EX", "HE", "MA", "SE"]

overlays = []
all_labels = []

fig, axes = plt.subplots(2, 2)
for i, ax in enumerate(axes.flatten()):
    label = load_label(image_path, label_types[i])
    all_labels.append(label)
    rgb = mask_label_overlay(mask, label)
    overlays.append(rgb)
    ax.imshow(rgb)
    ax.set_title(f"{label_types[i]}")
    ax.axis("off")

fig.tight_layout()
# %% [markdown]
# # Showing the segmentation / mask on top of the images
# We want to see which parts of the image the regions chosen correspond to
# %%
threshold = 0.01

# Create the masks
quac_mask = np.mean(mask, axis=0) > threshold
human_mask = np.sum(all_labels, axis=0) > 0
# Create the exclusions
# quac_only = np.logical_and(quac_mask, ~human_mask)
# human_only = np.logical_and(human_mask, ~quac_mask)
both = np.logical_and(quac_mask, human_mask)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
# Show the image
for ax, img, mask_type, title in zip(
    axes,
    [image, cf_image, image, image],
    [quac_mask, quac_mask, human_mask, both],
    ["QuAC", "QuAC - CF", "Human", "Both"],
):
    ax.imshow(img)
    ax.imshow(np.where(mask_type, np.nan, 0.5), cmap="gray", alpha=0.5)
    ax.axis("off")
    ax.set_title(title)

# %% [markdown]
# # Separating the types of annotations
# We'll make the same plot, but with contours drawn of different colors around
# the different types of human annotations in the "Human" and "Both" images.
# %%
fig, axes = plt.subplots(1, 4, figsize=(8, 20))
# Order will be QuAC, Both on the first row, then all four types on the second and third.

for ax, img, mask_type, title in zip(
    axes,
    [image, cf_image, image, image],
    [quac_mask, quac_mask, both, human_mask],
    ["QuAC", "QuAC-CF", "Both", "Human"],
):
    ax.imshow(img)
    ax.imshow(np.where(mask_type, np.nan, 0.5), cmap="gray", alpha=0.5)
    ax.axis("off")
    ax.set_title(title)
    # if "QuAC" in title:
    #     continue
    for i, label_type in enumerate(label_types):
        label = load_label(image_path, label_type)
        # get a contour of the label
        contours = np.logical_and(label, mask_type)
        ax.contour(contours, levels=[0.1], colors=[f"C{i}"], linewidths=1)
# Add the legend for the contour colors
from matplotlib.lines import Line2D

# Add the legend for the contour colors
legend_elements = [
    Line2D([0], [0], color=f"C{i}", lw=2, label=label_type)
    for i, label_type in enumerate(label_types)
]
fig.legend(handles=legend_elements, loc="center right", bbox_to_anchor=(1.01, 0.5))
# %% [markdown]
# # But what class was this?
# Let's have a look at the classification of the image.
# %%
prediction = report.predictions[order[0]]
print("Prediction:", np.argmax(prediction), prediction)

# %% [markdown]
# # Getting one of each
# Ideally, we want to see the best example for each (predicted) class.
# Let's pull those out and visualize them.
# %%
# Get the best example for each class
best_examples = {}
for i in range(1, 5):
    is_class = np.argmax(report.predictions, axis=1) == i
    index_in_class = np.argmax(np.array(report.quac_scores)[is_class])
    # Get the index in the full list
    index = np.where(is_class)[0][index_in_class]
    best_examples[i] = index
    print(f"Class {i}:", index, report.quac_scores[index])


# %%
def make_hybrid(image, cf_image, mask):
    # make the mask (h, w, c) instead of (c, h, w)
    mask = np.transpose(mask, (1, 2, 0))
    # Get normalized versions of the images
    image = np.array(image)
    cf_image = np.array(cf_image)
    image = (image - image.min()) / (image.max() - image.min())
    cf_image = (cf_image - cf_image.min()) / (cf_image.max() - cf_image.min())
    # Create hybrid
    hybrid = mask * cf_image + (1 - mask) * image
    # Prediction
    with torch.no_grad():
        hybrid_tensor = pre_classifier(hybrid).unsqueeze(0).float()
        hybrid_prediction = (
            softmax(classifier(hybrid_tensor)).detach().numpy().flatten()
        )
    # Turn hybrid into a PIL image
    hybrid = Image.fromarray((hybrid * 255).astype(np.uint8))
    return hybrid, hybrid_prediction


# %%
threshold = 0.01
# Plotting the best examples for each class
for class_index, index in best_examples.items():
    # Image
    image_path, cf_path = report.paths[index], report.target_paths[index]
    image, cf_image = Image.open(image_path), Image.open(cf_path)
    image, cf_image = transform(image), transform(cf_image)
    # Attribution
    attribution_path = report.attribution_paths[index]
    attribution = np.load(attribution_path)
    # Mask
    thresh = report.get_optimal_threshold(index)
    mask, _ = processor.create_mask(attribution, thresh)
    # Hybrid image
    hybrid, hybrid_prediction = make_hybrid(image, cf_image, mask)
    # TODO Hybrid classification?
    # Labels
    label_types = ["EX", "HE", "MA", "SE"]
    all_labels = []
    for label_type in label_types:
        label = load_label(image_path, label_type)
        all_labels.append(label)
    # Masks
    quac_mask = np.mean(mask, axis=0) > threshold
    human_mask = np.sum(all_labels, axis=0) > 0
    both = np.logical_and(quac_mask, human_mask)
    # Plotting
    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    for ax, img, mask_type, title in zip(
        axes,
        [image, hybrid, image, image],
        [quac_mask, quac_mask, human_mask, both],
        ["QuAC", "QuAC-CF", "Human", "Both"],
    ):
        ax.imshow(img)
        ax.imshow(np.where(mask_type, np.nan, 0.5), cmap="gray", alpha=0.5)
        ax.axis("off")
        ax.set_title(title)
        if "QuAC" in title:
            continue
        for i, label_type in enumerate(label_types):
            label = load_label(image_path, label_type)
            # get a contour of the label
            contours = np.logical_and(label, mask_type)
            ax.contour(contours, levels=[0.1], colors=[f"C{i}"], linewidths=1)
    # Add the legend for the contour colors
    legend_elements = [
        Line2D([0], [0], color=f"C{i}", lw=2, label=label_type)
        for i, label_type in enumerate(label_types)
    ]
    fig.legend(handles=legend_elements, loc="center right", bbox_to_anchor=(1.0, 0.5))
    fig.suptitle(f"Class {class_index}")
    plt.show()
    print("Hybrid prediction:", np.argmax(hybrid_prediction), hybrid_prediction)
# %% [markdown]
# # Saving images, etc for figure making
# For each of the best examples, we want to save the images, the masks, and the predictions.
#
# These will be saved in save_dir / "best_examples" / class_index
# We will make several images:
# - image.png - the original image
# - hybrid.png - the hybrid image
# We will also save the predictions to a CSV file.
# - format: filename,0,1,2,3,4,style (style is just "image" and "hybrid")
#
# Finally, we will save the masks as:
# - quac_mask.png
# - human_mask.png
# - both_mask.png
# - {label_type}_mask.png for each label type

# %%
from pathlib import Path

save_dir = Path("best_examples")
#
save_dir.mkdir(exist_ok=True)
# Iterate over the best examples
for class_index, index in best_examples.items():
    sub_dir = save_dir / f"{class_index}"
    # Make the subdirectory
    sub_dir.mkdir(exist_ok=True)
    # Image
    image_path, cf_path = report.paths[index], report.target_paths[index]
    image, cf_image = Image.open(image_path), Image.open(cf_path)
    image, cf_image = transform(image), transform(cf_image)
    # Image prediction
    prediction = report.predictions[index]
    # Attribution
    attribution_path = report.attribution_paths[index]
    attribution = np.load(attribution_path)
    # Mask
    thresh = report.get_optimal_threshold(index)
    mask, _ = processor.create_mask(attribution, thresh)
    # Hybrid image
    hybrid, hybrid_prediction = make_hybrid(image, cf_image, mask)
    # Labels
    label_types = ["EX", "HE", "MA", "SE"]
    all_labels = []
    for label_type in label_types:
        label = load_label(image_path, label_type)
        all_labels.append(label)
    # Masks
    quac_mask = np.mean(mask, axis=0) > threshold
    human_mask = np.sum(all_labels, axis=0) > 0
    both = np.logical_and(quac_mask, human_mask)
    # Save the images
    image.save(sub_dir / "image.png")
    hybrid.save(sub_dir / "hybrid.png")
    # Save the predictions
    with open(sub_dir / "predictions.csv", "w") as f:
        f.write("filename,0,1,2,3,4,style\n")
        f.write(f"image.png,{','.join(map(str, prediction))},image\n")
        f.write(f"hybrid.png,{','.join(map(str, hybrid_prediction))},hybrid")
    # Save the masks
    for mask_type, name in zip(
        [quac_mask, human_mask, both], ["quac_mask", "human_mask", "both_mask"]
    ):
        # NOTE We are saving the mask *inverse* for plotting in the paper
        mask = Image.fromarray((~mask_type * 255).astype(np.uint8))
        mask.save(sub_dir / f"{name}.png")
    for label_type in label_types:
        label = load_label(image_path, label_type)
        not_label = ~(label > 0)
        # NOTE We are saving the mask *inverse* for plotting in the paper
        mask = Image.fromarray((not_label * 255).astype(np.uint8))
        mask.save(sub_dir / f"{label_type}_mask.png")
    print("Saved to", sub_dir)

# %% [markdown]
# # Plotting the attributions
# What happens if we just plot the attributions for the best examples?
# %%
from scipy import ndimage

for class_index, index in best_examples.items():
    # Image
    image_path, cf_path = report.paths[index], report.target_paths[index]
    image, cf_image = Image.open(image_path), Image.open(cf_path)
    image, cf_image = transform(image), transform(cf_image)
    # Attribution
    attribution_path = report.attribution_paths[index]
    attribution = np.load(attribution_path)
    fig, axes = plt.subplots(1, 2)
    # Plotting the "max" attribution
    max_attr = np.max(attribution, axis=0)
    filtered_attr = ndimage.gaussian_filter(max_attr, sigma=2)
    axes[0].imshow(filtered_attr, vmin=0, vmax=1, cmap="Reds")
    # Plotting the human labels
    label_types = ["EX", "HE", "MA", "SE"]
    all_labels = []
    for label_type in label_types:
        label = load_label(image_path, label_type)
        all_labels.append(label)
    axes[1].imshow(np.sum(all_labels, axis=0), cmap="Reds")
    axes[0].axis("off")
    axes[1].axis("off")
    fig.suptitle(f"Class {class_index}")
    plt.show()

# %% [markdown]
# ## Saving the overlay
# We might want to use these overlay images in the future.
# We will therefore save them to file.
#
# We will also need an overlay for the summed labels, so we will create that as well.
# %%
# TODO uncomment, later
# for label, overlay in zip(label_types, overlays):
#     overlay_path = figure_path / f"{label}_overlay.png"
#     Image.fromarray(overlay).save(overlay_path)

# # Summed labels
# overlay = mask_label_overlay(mask, np.sum(all_labels, axis=0))
# Image.fromarray(overlay).save(figure_path / "summed_overlay.png")
# print("Saved overlays to", figure_path)
# %% [markdown]
# ## Getting the detection score overall
#
# Let's get the precision and recall scores, per label, for each sample.
#
# We also want to see how much the classifier is looking at that is *not* in the lesion segmentation given by the expert.
# In this case, it doesn't matter *which* label, so we will use the summed labels.
#
# In order to get these metrics, we need to choose a threshold for the mask.
# In this case, we have chosen 0.1 as a threshold (manually), as this recovers what we consider
# as being "in" the mask. As shown below.
# %%
fig, ax = plt.subplots(1, 2)
ax[0].imshow(np.mean(mask, axis=0))
ax[1].imshow(np.mean(mask, axis=0) > 0.1)

ax[0].axis("off")
ax[1].axis("off")
# %% [markdown]
# Feel free to change this threshold to see how it affects the metrics.
# %%
from tqdm import tqdm
from utils import iou, precision, recall

precisions = {label_type: [] for label_type in label_types}
recalls = {label_type: [] for label_type in label_types}
f1_scores = {label_type: [] for label_type in label_types}

mask_sizes = []
for i in tqdm(range(len(report.paths))):
    attribution_path = report.attribution_paths[i]
    attribution = np.load(attribution_path)

    thresh, idx = report.get_optimal_threshold(i, return_index=True)
    mask, _ = processor.create_mask(attribution, thresh)
    mask_sizes.append(report.normalized_mask_sizes[i][idx])
    for j, label_type in enumerate(label_types):
        label = load_label(image_path, label_type)
        p = precision(mask, label, thresh=0.1)
        r = recall(mask, label, thresh=0.1)
        f1 = (2 * p * r) / max(p + r, 1)
        f1_scores[label_type].append(f1)
        precisions[label_type].append(p)
        recalls[label_type].append(r)

# %% [markdown]
# ## Plotting the Precision and Recall
# We will plot the precision and recall for each label type.
#
# A high recall means that the manual segmentation is well covered by the mask.
# A low precision means that the classifier is *also* looking at things that are not in the manual segmentation.
# %%
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
ax1.hist(list(recalls.values()), bins=20, alpha=0.5, label=label_types)
ax1.set_xlabel("Recall")
ax1.set_ylabel("Count")
ax2.hist(list(precisions.values()), bins=20, alpha=0.5, label=label_types)
ax2.set_xlabel("Precision")
ax2.set_ylabel("Count")
plt.legend()
# %% [markdown]
# ## Saving the precision and recall
# We will save the precision and recall to a CSV file, so that we can use it in the future.
# %%
# precision_recall_file = figure_path / "precision_recall.csv"
# with open(precision_recall_file, "w") as f:
#     f.write("label_type,precision,recall,f1\n")
#     for label_type in label_types:
#         for p, r, f1 in zip(
#             precisions[label_type], recalls[label_type], f1_scores[label_type]
#         ):
#             f.write(f"{label_type},{p},{r},{f1}\n")

# print("Saved precision and recall to", precision_recall_file)
# %% [markdown]
# ## Plotting mask sizes
# %%
plt.hist(mask_sizes, bins=20, alpha=0.5)
plt.xlabel("Mask size")
plt.ylabel("Count")
plt.show()
# %%
plt.scatter(mask_sizes, report.quac_scores)

# %%
